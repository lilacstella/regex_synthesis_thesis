%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Template code for the Undergraduate Research Scholars thesis program starting, updated by Undergraduate Research Scholars program staff. Version 6.0. Last Updated: Fall 2024
%  Modified by Tawfik Hussein from the template code for TAMU Theses and Dissertations starting Spring 2018, authored by Sean Zachary Roberson. Version 3.17.09.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           SECTION IV: CONCLUSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%________(0)__________
% Do not modify. This is the page heading.


% THIS LINE ADDS THE CONCLUSION TO THE TABLE OF CONTENTS
%\addcontentsline{toc}{chapter}{\vspace{1.0em}\hspace{1.0em} IV.\hspace{2em}CONCLUSION} 


%__________(1)_________
% Modification Needed!

% THIS IS THE SECTION WHERE YOU TYPE IN THE TEXT RELATED TO YOUR CONCLUSION. NOTICE THE DOUBLE \indent COMMAND THAT PROPERLY INDENTS THE BEGINNING OF EACH PARAGRAPH

\chapter{CONCLUSION AND FUTURE WORK}

\section{Conclusion}

\indent\indent This thesis explored how different synthesis methodologies perform in the constrained task of regex generation. By comparing AlphaRegex, the L* algorithm, and LLMs, we have answered the central research question: how do these approaches compare in correctness, efficiency, and complexity—and could they be combined for better outcomes?

\indent\indent AlphaRegex demonstrates the strength of heuristic search, particularly when weights are carefully tuned. Its flexibility lies in its cost function, which allows for prioritizing minimal length or computation time. However, this comes at the cost of unpredictable performance across benchmarks—too low a hole penalty may produce verbose or invalid solutions, while too high a penalty can lead to excessive runtimes or failure to converge.

\indent\indent The L* algorithm offers a formally grounded approach with guaranteed correctness under a proper teacher. Its output, derived from DFAs, is provably equivalent to the target language, but often lacks elegance or conciseness. Additionally, it is sensitive to the formulation of counterexamples and can be inefficient in practice on complex languages.

\indent\indent LLMs represent a fundamentally different paradigm: they do not search or construct programs in a structured way but instead rely on pattern recognition and learned syntax. Their performance is impressive on benchmarks resembling seen examples, yet fragile when compositional logic or negation is required. We observed degradation in performance as benchmark complexity increased, along with drift from user constraints in long prompting sessions.

\indent\indent Together, these findings suggest that no single method dominates; rather, each has a niche. Chaining methods—for instance, using LLMs for initial guesses, refined by AlphaRegex or verified through L*—offers a promising avenue. Our experiments with heuristic adjustments in AlphaRegex showed that even modest changes in search prioritization can lead to trade-offs in correctness and output readability, highlighting the importance of interpretability and controllability in synthesis.

\indent\indent From a broader perspective, this work underscores the importance of test-driven synthesis. The quality of input-output examples or formal constraints fundamentally shapes the success of all three methods. We advocate for further research into human-in-the-loop tools that help users craft effective specifications, guided by intelligent feedback.

\section{Future Work}

\indent\indent From here, there are many potential directions for future work. The following list outlines some of the most promising avenues:

\begin{itemize}
  \item \textbf{Human-in-the-loop synthesis:} Building tools that help users generate, evaluate, and refine test cases could greatly improve synthesis outcomes. Future systems might include example-suggestion engines, test coverage visualizations, or even learning-based test generators.

  \item \textbf{Enhanced chaining strategies:} This work offers only preliminary results in chaining LLMs with traditional synthesizers. A more robust pipeline could involve using LLMs to draft regex, AlphaRegex to refine structure, and L* to verify equivalence.

  \item \textbf{Generalization to other DSLs:} Although this thesis focused on regex, many insights apply to broader program synthesis contexts, including configuration synthesis, data transformation scripts, or query generators.

  \item \textbf{Weight learning and adaptation:} In AlphaRegex, we relied on static weight configurations. Future work could explore meta-learning or reinforcement learning to dynamically tune weights based on benchmark structure.

  \item \textbf{Prompt stability in LLMs:} The degradation observed across long sessions raises the need for tooling or architecture-level improvements to preserve intent and context across multiple invocations.
\end{itemize}

\indent\indent Ultimately, this thesis contributes both a comparative framework and practical implementations of three representative synthesis strategies. It lays the groundwork for building future systems that combine the controllability of symbolic methods with the flexibility of neural networks, all while centering the human user's intent through better test design and feedback tools.




